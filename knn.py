# -*- coding: utf-8 -*-
"""KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VRESD0G0KGZg_nzKhqeQsPFWxjnODvFg
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

train_data = pd.read_excel("train.xlsx")
train_X = train_data.drop("Label", axis=1).values
train_y = train_data["Label"].values

test_data = pd.read_excel("test.xlsx")
test_X = test_data.drop("Label", axis=1).values
test_y = test_data["Label"].values

##implementing kNN classifier :
##using Euclidean distance:
def calculate_euclidean_distance(point1, point2):
    squared_distance = np.sum(np.square(point1 - point2))
    euclidean_distance = np.sqrt(squared_distance)
    return euclidean_distance

def knn_classifier_euclidean(train_features, train_labels, test_features, k):
    predictions = []
    for test_instance in test_features:
        distances = [calculate_euclidean_distance(train_instance, test_instance) for train_instance in train_features]
        nearest_indices = np.argsort(distances)[:k]
        nearest_labels = train_labels[nearest_indices]
        unique_labels, label_counts = np.unique(nearest_labels, return_counts=True)
        predicted_label = unique_labels[np.argmax(label_counts)]
        predictions.append(predicted_label)
    return predictions

#using consine similarity
def cosine_similarity(vector1, vector2):
    dot_product = np.dot(vector1, vector2)
    norm_vector1 = np.linalg.norm(vector1)
    norm_vector2 = np.linalg.norm(vector2)
    similarity = dot_product / (norm_vector1 * norm_vector2)
    return similarity

def knn_classifier_cosine(train_features, train_labels, test_features, k):
    predictions = []
    for test_instance in test_features:
        similarities = [cosine_similarity(test_instance, train_instance) for train_instance in train_features]
        nearest_indices = np.argsort(similarities)[-k:]
        nearest_labels = train_labels[nearest_indices]
        unique_labels, label_counts = np.unique(nearest_labels, return_counts=True)
        predicted_label = unique_labels[np.argmax(label_counts)]
        predictions.append(predicted_label)
    return predictions

def calculate_metrics(test_labels, predictions):
    correct = 0
    total = len(test_labels)
    unique_labels = set(test_labels)
    precision = 0
    recall = 0

    for i in range(total):
        if test_labels[i] == predictions[i]:
            correct += 1

    for label in unique_labels:
        true_positive = 0
        false_positive = 0
        false_negative = 0

        for i in range(total):
            if test_labels[i] == label and predictions[i] == label:
                true_positive += 1
            elif test_labels[i] != label and predictions[i] == label:
                false_positive += 1
            elif test_labels[i] == label and predictions[i] != label:
                false_negative += 1

        precision += true_positive / (true_positive + false_positive)
        recall += true_positive / (true_positive + false_negative)

    accuracy = correct / total
    precision /= len(unique_labels)
    recall /= len(unique_labels)
    f_score = (2 * precision * recall) / (precision + recall)

    return accuracy*100,precision*100,recall*100,f_score*100

# Using Euclidean distance
euclidean_predictions = knn_classifier_euclidean(train_X, train_y, test_X, k=3)

euclidean_accuracy, euclidean_precision, euclidean_recall, euclidean_f_score = calculate_metrics(test_y, euclidean_predictions)

print("Euclidean Distance Results:")
print("Accuracy:", euclidean_accuracy)
print("Precision:", euclidean_precision)
print("Recall:", euclidean_recall)
print("F-score:", euclidean_f_score)

# Using cosine similarity

cosine_predictions = knn_classifier_cosine(train_X, train_y, test_X, k=3)
cosine_accuracy, cosine_precision, cosine_recall, cosine_f_score = calculate_metrics(test_y, cosine_predictions)

print("Cosine Similarity Results:")
print("Accuracy:", cosine_accuracy)
print("Precision:", cosine_precision)
print("Recall:", cosine_recall)
print("F-score:", cosine_f_score)

def accuracy_score(true_labels, predicted_labels):
    correct = 0
    total = len(true_labels)

    for true_label, predicted_label in zip(true_labels, predicted_labels):
        if true_label == predicted_label:
            correct += 1

    accuracy = correct / total
    return accuracy*100

# Varying the value of k and evaluating performance
k_values = [1, 2, 3, 4, 5, 6, 7, 8, 9]
accuracy_scores_euclidean = []
accuracy_scores_cosine = []

for k in k_values:
    predictions_euclidean = knn_classifier_euclidean(train_X, train_y, test_X, k)
    accuracy_euclidean = accuracy_score(test_y, predictions_euclidean)
    accuracy_scores_euclidean.append(accuracy_euclidean)
    
    predictions_cosine = knn_classifier_cosine(train_X, train_y, test_X, k)
    accuracy_cosine = accuracy_score(test_y, predictions_cosine)
    accuracy_scores_cosine.append(accuracy_cosine)

# Plotting the results
plt.plot(k_values, accuracy_scores_euclidean, label='Euclidean Distance')
plt.plot(k_values, accuracy_scores_cosine, label='Cosine Similarity')
plt.xlabel('k')
plt.ylabel('Classification Accuracy')
plt.title('KNN Classifier Performance')
plt.legend()
plt.show()

def normalize_features(features):
    normalized_features = []
    for instance in features:
        length = np.linalg.norm(instance)
        normalized_instance = instance / length
        normalized_features.append(normalized_instance)
    return normalized_features

# Normalize the train and test feature vectors
normalized_train_features = normalize_features(train_X)
normalized_test_features = normalize_features(test_X)

# Use the normalized feature vectors for classification
predictions_normalized = knn_classifier_euclidean(normalized_train_features, train_y, normalized_test_features, k)

# Calculate accuracy with normalized feature vectors
accuracy_normalized = accuracy_score(test_y, predictions_normalized)

# Print the accuracy
print("Accuracy with normalized feature vectors:", accuracy_normalized)